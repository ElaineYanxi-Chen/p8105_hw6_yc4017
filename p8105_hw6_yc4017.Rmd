---
title: "p8105 Homework 6"
author: "Elaine Yanxi Chen"
date: "`r Sys.Date()`"
output: github_document
---

## Packages and settings

First we load the packages necessary to knit this document.

```{r packages and settings, message = FALSE}
library(tidyverse)
library(mgcv)
library(modelr)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```


## Problem 1: Central Park weather data

First we will load the data.

```{r download data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

In this problem, we will focus on a simple linear regression model with `tmax` as the response and `tmin` as the predictor. We are also interested in the distribution of two quantities estimated from these data: $\hat{r}^2$ and $\log(\beta_0 * \beta1)$. 

Here are the steps to obtain a distribution for $\hat{r}^2$ 

* draw bootstrap samples using `modelr::boostrap`
* a model to each
* extra values with `broom::glance`
* summarize

```{r bootstrap}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

To construct the 95% confidence interval, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 

## Problem 2: Homicides in Baltimore, MD

### Data Import

After loading the dataset, we want to do some initial cleaning and tidying following these criteria below:

* We want to load the data on homicides in 50 large U.S. cities. 
* We want to create a `city_state` variable and a binary variable indicating whether the homicide is solved.
* Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO since they do not report victim race. Omit Tulsa, AL because this is a data entry mistake.
* We will limit the analysis for whom `victim_race` is `white` or `black`.
* Make sure that `victim_age` is a numeric variable.

```{r homicide_df}
homicide_df = 
  read_csv("data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, state, sep = ","),
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")
  ) %>% 
  filter(
    !city_state %in% c("Dallas,TX", "Phoenix,AZ", "Kansas City,MO", "Tulsa,AL"),
    victim_race %in% c("White", "Black"))
```


### Baltimore, MD

We are interested in using the `glm` function to fit a logistic regression model with resolved vs unresolved as the outcome, and victim age, sex, and race as predictors.

To do so, we will first generate a dataset containing these variables only.

```{r baltimore_df}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore,MD") %>% 
  select(resolved, victim_age, victim_race, victim_sex)
```

Now we will run the logistic regression model using `glm` and save the output as an R object.

```{r fit_logistic}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) 
```

```{r clean glm_baltimore}
glm_baltimore = 
  fit_logistic %>% 
  broom::tidy() %>% 
  janitor::clean_names() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std_error),
    CI_upper = exp(estimate + 1.96 * std_error)) %>% 
  select(-c(std_error, statistic)) 

glm_baltimore %>% 
  knitr::kable(digits = 3)
```

After running the logistic regression model, we want to obtain the estimate and the confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed. 

```{r victim sex}
glm_baltimore %>% 
  filter(term == "victim_sexMale") %>% 
  select(OR:CI_upper) %>% 
  knitr::kable(
    digits = 3,
    col.names = c("Odds Ratio", "Lower 95% CI", "Upper 95% CI"))
```

We see that keeping all other variables fixed, homicides in which the victim is male are 57.4% less likely to be resolved than those in which the victim is female. We are 95% confident that the true odds ratio for the association between victim's sex and resolved homicides lies between 0.325 and 0.558. 


### Iterating `glm` for each city

Now we want to run `glm` for each of the cities in the datset.

First, we will create a dataset containing all cities but only with variables of interest.

```{r homicide_glm_df}
homicide_glm_df =
  homicide_df %>% 
  select(city_state, resolved, victim_age, victim_race, victim_sex)
```

Now we will map the `glm` function across all cities, tidy the output, and finally filter for `victim_sex` and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 

```{r glm iteration}
nest_glm_city =
  homicide_glm_df %>% 
  nest(df = -city_state) %>% 
  mutate(
    models = map(.x = df, ~glm(resolved ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results)

glm_city =
  nest_glm_city %>% 
  janitor::clean_names() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std_error),
    CI_upper = exp(estimate + 1.96 * std_error)) %>% 
  select(-c(std_error, statistic)) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, OR:CI_upper) 

glm_city %>% 
  knitr::kable(
    digits = 3,
    col.names = c("City, State", "Odds Ratio", "Lower 95% CI", "Upper 95% CI"))
```


### Plot for estimated ORs and CIs for each city

Lastly, we want to create a plot showing the estimated ORs and CIs for each city. We will also show it in a way so that cities are organized according to the their estimated OR. 

```{r glm_plot}
glm_plot = 
  glm_city %>% 
  mutate(
    city_state = fct_reorder(city_state, OR, .desc = TRUE)
  ) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point(alpha = 0.5) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1)) +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper),
                alpha = 0.5) +
  labs(
    title = "Estimated Odds Ratios and 95% Confidence Intervals of Solved Homicides and 
    Victim Sex for Large U.S.Cities",
    x = "City, State",
    y = "Estimates Odds Ratios (95% Confidence Intervals)"
  ) + 
  geom_hline(yintercept = 1, colour = "red")

glm_plot
```

Examining the plot, here are a few comments:

* The highest odds ratio was observed in Albuquerque, NM, and the lowest odds ratio was observed in New York, NY.
* For most large US cities, homicides with female victims have higher odds of being resolved than those with male victims. A few cities are exceptions where homicides with male victims were more likely to be resolved: Albuquerque, NM; Stockton, CA; Fresno, CA; Nashville, TN; Richmond, VA. One thing to note is that the ORs for several of these cities had very wide 95% CIs and the results were statistically insignificant since the 95% CIs cross the null value of OR = 1.
* Larger urban centres generally have OR < 1 with narrower 95% CIs, possibly due to a larger sample size from a larger population. Some examples include New York, NY; Chicago, IL; Pittsburgh, PA; San Francisco, CA.


## Problem 3: Child's Birthweight

In this problem, we will analyze data gathered on child's birthweight and several variables.

### Data import and cleaning

We will first load the data and inspect it, before cleaning it for regression analysis.

For certain variables such as `babysex` and `frace`, we will convert them from numeric to factor so that we can easily put them into regression models. In addition, we will check for missing data.

```{r load childbw}
childbw_df = 
  read_csv("data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )

sum(is.na(childbw_df))
```

From our intial analysis, it seems that there is no missing data in this child birthweight dataset since the count for missing data is `r sum(is.na(childbw_df))`.

### Regression model for birthweight

First, we want to build a regression model for birthweight based on hypothesized theories and/or data-driven model-building process. 

I decide to research literature for known risk factors of low birthweight and compose my regression model as such. Based on this [article](https://www.cedars-sinai.org/health-library/diseases-and-conditions---pediatrics/v/very-low-birth-weight.html), I have decided to include the following risk factors:

* Mother's weight gain during pregnancy in pounds (`wtgain`)
* Previous number of low birth weight babies (`pnumlbw`)
* Average number of cigarettes smoked per day during pregnancy (`smoken`)
* Race of parents (`frace` and `mrace`)
* Mother's age at delivery in years (`momage`)

Of course there are other potential factors from this dataset that could affect low birthweight, but I do not want to include too many variables due to concerns about sample size.

```{r build model}
own_mod = lm(bwt ~ wtgain + pnumlbw + smoken + frace + mrace + momage, data = childbw_df)
```

We want to show a plot of model residuals against fitted values. In order to do so, we will use `add_predictions` and `add_residuals` to make this plot. 

```{r own_mod plot}
childbw_df %>% 
  modelr::add_predictions(own_mod) %>% 
  modelr::add_residuals(own_mod) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point()
```

Looking at the residual plot, there doesn't seem to be any non-random pattern to the plot, hence we might not be able to add another predictor to the model.

### Comparison with two other models

First, we will build the two given models, one with main effects of length at birth (`blength`) and gestational age (`gaweeks`) ,and the other with main effects and all interactions of head circumference (`bhead`), length (`blength`), and sex (`babysex`).

```{r given models}
main_mod = lm(bwt ~ blength + gaweeks, data = childbw_df)

int_mod = lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = childbw_df)
```

We will then make the comparison in terms of the cross-validated prediction error. To do so, we will use the functions including `crossv_mc` and those in `purrr`. 

```{r crossv_mc}
cv_df =
  crossv_mc(childbw_df, 100) 

cv_df = 
  cv_df %>% 
  mutate(
    own_mod = map(train, ~lm(bwt ~ wtgain + pnumlbw + smoken + frace + mrace + momage, data = .x)),
    main_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    int_mod = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = .x))
  ) %>% 
  mutate(
    rmse_own = map2_dbl(own_mod, test, ~rmse(model = .x, data = .y)),
    rmse_main = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
    rmse_int = map2_dbl(int_mod, test, ~rmse(model = .x, data = .y))
  )
```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(
    title = "Vionlin Plot Comparing Three Regression Models for Child's Birthweight",
    x = "Model Name",
    y = "Root Mean Squared Errors (RMSEs)"
  )
```

Wow...my model is crap...